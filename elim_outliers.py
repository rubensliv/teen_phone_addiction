'''
Descri√ß√£o Geral da Base de Dados: teen_phone_addiction_dataset.csv
Essa base parece estar relacionada ao n√≠vel de v√≠cio em celular entre adolescentes. 
Foi constru√≠da a partir de question√°rios, entrevistas ou registros comportamentais de jovens, 
com o objetivo de prever o grau de depend√™ncia do celular com base em diversos fatores.

üîç Estrutura esperada da base de dados

‚úÖ Colunas que foram removidas:
ID: identificador √∫nico de cada adolescente ‚Äî sem utilidade para predi√ß√£o.

Name: nome da pessoa ‚Äî tamb√©m irrelevante para o modelo.

üéØ Vari√°vel alvo (target):

Addiction_Level: n√≠vel de depend√™ncia do celular (provavelmente num√©rico e cont√≠nuo).
Ex: pode ir de 0 a 10, onde 0 = sem depend√™ncia, 10 = alto v√≠cio.

üì• Vari√°veis preditoras (features):

O restante das colunas foi usado para prever o Addiction_Level. Devem incluir:

Tipo de vari√°vel	Exemplos poss√≠veis

Categ√≥ricas	        G√™nero, Tipo de escola, Ocupa√ß√£o dos pais
Num√©ricas	        Horas no celular por dia, notas escolares, idade
Comportamentais	    Frequ√™ncia de uso de redes sociais, jogos, etc.

Essas vari√°veis foram:

Codificadas com LabelEncoder (as categ√≥ricas).

Normalizadas com StandardScaler (as num√©ricas).

Filtradas por outliers usando a t√©cnica do IQR.


Este programa est√° estruturado para resolver uma tarefa de regress√£o supervisionada com o
algoritmo XGBoost, que √© altamente eficaz e usado em competi√ß√µes de Machine Learning.

‚úÖ Pontos fortes:

Realiza todo o pipeline: leitura, limpeza, codifica√ß√£o, divis√£o dos dados, treinamento, avalia√ß√£o e
interpreta√ß√£o.

Usa o LabelEncoder  para lidar com vari√°veis categ√≥ricas automaticamente.

Avalia o modelo com m√©tricas apropriadas (RMSE e R¬≤).

Apresenta visualmente a import√¢ncia das features ‚Äî √∫til para an√°lise interpretativa.


‚ö†Ô∏è Sugest√µes de melhorias opcionais:

Normaliza√ß√£o de vari√°veis: pode melhorar a performance, principalmente se adicionar outros algoritmos
al√©m do XGBoost.

Valida√ß√£o cruzada (cross-validation): para obter resultados mais robustos e evitar overfitting.

GridSearch ou RandomizedSearch: para ajustar hiperpar√¢metros do XGBoost.

Salvar o modelo treinado com joblib ou pickle se for usar em produ√ß√£o.

'''

# Importa bibliotecas essenciais
import pandas as pd                              # Para manipula√ß√£o de dados tabulares
import xgboost as xgb                            # Algoritmo XGBoost para regress√£o
from sklearn.model_selection import train_test_split  # Para dividir os dados em treino e teste
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error  # Avalia√ß√£o do modelo
from sklearn.preprocessing import LabelEncoder   # Para transformar vari√°veis categ√≥ricas em n√∫meros
import matplotlib.pyplot as plt                  # Para visualizar a import√¢ncia das vari√°veis
import numpy as np                               # Para opera√ß√µes num√©ricas        

# L√™ o dataset do arquivo CSV
df = pd.read_csv("teen_phone_addiction_dataset.csv")  # Certifique-se de que o arquivo esteja no mesmo diret√≥rio

# Remove colunas que n√£o contribuem para o modelo (como ID e Name)
df.drop(columns=['ID', 'Name'], inplace=True)

# Identifica automaticamente as colunas do tipo "object" (texto/categ√≥ricas)
categorical_cols = df.select_dtypes(include='object').columns

# Converte cada coluna categ√≥rica em valores num√©ricos usando LabelEncoder
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))  # Garante que todos os valores sejam tratados como string

# Remove qualquer linha que contenha valores ausentes (NaN)
df.dropna(inplace=True)

# -------------------------------------------------------------------------------------
# NOVO TRECHO: Remo√ß√£o de outliers usando a t√©cnica do intervalo interquartil (IQR)
# -------------------------------------------------------------------------------------

# Calcula os limites inferior e superior para cada coluna num√©rica (exceto a vari√°vel alvo)
Q1 = df.quantile(0.25)  # Primeiro quartil
Q3 = df.quantile(0.75)  # Terceiro quartil
IQR = Q3 - Q1           # Intervalo interquartil

# Cria um filtro booleano que mant√©m apenas os dados dentro do intervalo interquartil
# (valores entre Q1 - 1.5*IQR e Q3 + 1.5*IQR s√£o considerados aceit√°veis)
df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]

# Coment√°rio:
# A linha acima elimina todas as linhas que possuem ao menos um valor considerado outlier
# com base na t√©cnica do IQR. Isso √© √∫til para evitar que valores extremos influenciem o modelo.
# -------------------------------------------------------------------------------------

# Define a vari√°vel alvo que queremos prever
target = 'Addiction_Level'

# Define a lista de vari√°veis independentes (todas as colunas exceto a vari√°vel alvo)
features = [col for col in df.columns if col != target]

# Cria os conjuntos de entrada (X) e sa√≠da (y)
X = df[features]
y = df[target]

# Separa os dados em treino (80%) e teste (20%) de forma aleat√≥ria, com uma semente fixa para reprodutibilidade
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Cria o modelo de regress√£o XGBoost com objetivo de minimizar o erro quadr√°tico
model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)

# Treina o modelo com os dados de treino
model.fit(X_train, y_train)

# Realiza previs√µes com os dados de teste
y_pred = model.predict(X_test)

# -------------------------------------------------------------------------------------
# Avalia√ß√£o do modelo com diferentes m√©tricas de regress√£o
# -------------------------------------------------------------------------------------

# Calcula o Erro M√©dio Quadr√°tico (RMSE)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

# Calcula o Erro Absoluto M√©dio (MAE)
mae = mean_absolute_error(y_test, y_pred)

# Calcula o coeficiente de determina√ß√£o (R¬≤)
r2 = r2_score(y_test, y_pred)

# Exibe as m√©tricas de avalia√ß√£o do modelo
print(f"RMSE: {rmse:.2f}")     # Erro m√©dio quadr√°tico
print(f"MAE: {mae:.2f}")       # Erro absoluto m√©dio
print(f"R¬≤: {r2:.2f}")         # Coeficiente de determina√ß√£o

# Gera um gr√°fico com a import√¢ncia relativa de cada vari√°vel preditora
xgb.plot_importance(model)
plt.title("Import√¢ncia das vari√°veis")
plt.show()
